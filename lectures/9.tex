\chapter{Кластеризация}

Лектор: Сергей Борисович Муравьёв

\section{Базовая постановка задачи кластеризации}

Задача классификации состоит в том, чтобы разделить набор объектов на группы так, чтобы объекты в этих группах имели похожие свойства. Похожесть формализуется мерой.

\begin{definition}
    Пусть задан набор данных $D$, состоящий из объектов на $X$, $\rho:X\times X\to[0;+\infty)$ --- метрика на $X$. Тогда \textbf{задача кластеризации} состоит в поиске алгоритма $\alpha:X\to Y$, где $Y$ -- множество кластеров. Выделяют два вида кластеризации:
    \begin{itemize}
        \item \textbf{Жесткая кластеризация} --- объект принадлежит ровно одному кластерам.
        \item \textbf{Мягкая кластеризация} --- объект принадлежит одному или многим кластерам.
    \end{itemize}
\end{definition}

Заметим, что данная постановка задачи математически некорректна по следующим причинам:
\begin{enumerate}
    \item Нет правильной постановки задачи
    \item Нет универсального критерия качества
    \item Нет универсальной меры расстояния между объектами (следствие теоремы Клейнберга)
    \item Число кластеров обычно неизвестно
\end{enumerate}

Примеры кластеров:
\begin{itemize}
    \item Явно разделимые
    \item Полосы
    \item С "мостами"
    \item С шумами
    \item Смесь распределений
    \item Нет "кластеров"
\end{itemize}

Практическое применение кластеризации:
\begin{itemize}
    \item Биология и медицина:
    \begin{itemize}
        \item Анализ последовательностей
        \item Медицинская визуализация (КТ и МРТ)
    \end{itemize}
    \item Социальные науки
    \begin{itemize}
        \item Анализ криминала
    \end{itemize}
    \item Информационные технологии:
    \begin{itemize}
        \item Сегментация изображения
    \end{itemize}
    \item Маркетинг:
    \begin{itemize}
        \item Целевые группа
    \end{itemize}
    \item Анализ текста
    \item Социальные сети:
    \begin{itemize}
        \item Выделение сообществ
    \end{itemize}
\end{itemize}

\section{Оценка качества кластеризации}

\begin{definition}
    \textbf{Внутренние меры} --- меры, которые не используют внешней информации о структуре разбиения (только $X$ и построенное разбиение $\widehat{Y}$. Примеры:
    \begin{itemize}
        \item Компактность кластеров (Cluster Cohesion)
        \item Отделимость кластеров (Cluster separation)
        \item Индекс Дана (Dunn Index) и его вариации
        \item Индекс $S_{dbw}$
        \item Силуэт (Silhouette)
        \item Индекс Calinski-Harabasz
        \item Индекс C
        \item Индекс Дэвида-Болдуина (Davies-Bouldin Index)
        \item Score function
        \item Индекс $\gamma$
        \item Индекс COP
        \item Индекс CS
        \item Индекс Sym и его вариации
        \item Индекс SV
        \item Индекс OS
    \end{itemize}
\end{definition}

\begin{definition}
    \textbf{Внешние меры} --- меры, которые используют для оценки $Y$ и $\widehat{Y}$. Примеры:
    \begin{itemize}
        \item Модифицированные TP, FP, FN, TN
        \item Индекс Rand
        \item Индекс Adjusted Rand
        \item Индекс Жаккара (Jaccard Index)
        \item Индекс Фоулкса-Мэллова (Fowlkes-Mallows Index)
        \item Hubert Г statistic
        \item Индекс $\phi$
        \item Minkowski Score
        \item Индекс Гудмэна-Крускала (Goodman-Kruscal Index)
        \item Purity
        \item Variation of Information
    \end{itemize}
\end{definition}

\begin{remark}
    Не рекомендуется использовать внешние меры для оценки алгоритмов.
\end{remark}

\section{Эвристические алгоритмы кластеризации}

\subsection{k-Means}

\begin{definition}
    Метод K-средних (Алгоритм Ллойда) --- метод решения задачи кластеризации. Работает по следующему алгоритму:
    \begin{enumerate}
        \item Выбираются K центров случайным образом.
        \item Точка относится к тому кластеру, к центру которого она ближе всего.
        \item Рассчитываются средние по координатам точек для каждого кластера.
        \item Центры кластеров перемещаются к средним координатам.
        \item Повторяются шаги 2 - 4, пока центры кластеров не перестанут перемещаться.
    \end{enumerate}
\end{definition}

Подводные камни:
\begin{itemize}
    \item Неправильный выбор числа кластеров.
    \item Неправильная инициализация (выбор случайных центров).
\end{itemize}

\begin{definition}
    \textbf{Метод локтя} (\textit{elbow method}) --- алгоритм выбора числа кластеров: строится зависимость суммы квадратов расстояний от каждой точки до ближайшего центра и зависимости от $k$ -- числа кластеров, а когда график выходит на плато, можно считать, что найдено неплохое значение $k$.
\end{definition}

\begin{remark}
    На практике иногда на графике не видно локтя.
\end{remark}

\begin{definition}
    \textbf{Silhoutte Analysis} (\textit{анализ силуэтов}) --- алгоритм при котором для каждой точки из набора данных рассчитывается среднее расстояние до точек своего кластера $a^i$, среднее расстояние до точек ближайшего другого кластера $b^i$ и коэффициент:
    \[
        \dfrac{b^i-a^i}{\max(a^i,b^i)}
    \]
    Значение коэффициента в районе 0 означает, что точка на границе кластеров, в районе 1 -- точка глубоко в своем кластере, меньше 0 -- точка неправильно кластеризована.
\end{definition}

Недостатки k-Means:
\begin{enumerate}
    \item Кластеры стремятся быть в форме шара (многоугольники Вороного)
    \item Кластеры стремятся быть одинакового размера.
\end{enumerate}

Таким образом, k-Means -- это быстрый и простой алгоритм всего лишь с одним гиперпараметром (хотя наличие гиперпараметра само по себе уже минус), однако он не умеет обрабатывать кластеры сложной формы и содержит случайность, а значит, может выдавать разные результаты при нескольких запусках. Де-факто является самым часто применяемым алгоритмом для анализа данных, смело может быть использован в качестве бейз-лайна.

\begin{remark}
    Можно сдвигать центры не к среднему, а к медиане. Такой метод называется k-Medians. Он более устойчив к выбросам, однако скорость его работы падает при большом количестве данных.
\end{remark}

\subsection{Mean Shift}

\begin{definition}
    \textbf{Mean-Shift Clustering} -- метод решения задачи кластеризации. Работает по следующему алгоритму:
    \begin{enumerate}
        \item Выбирается окно радиусом $R$.
        \item Окно сдвигается в сторону центра точек, которые в него попали.
        \item Окно двигается в сторону уплотнения точек, пока не достигает вершины.
        \item Таких окон очень много, их инициализируют сеткой, чтобы каждая точка попадала хотя бы в окно.
        \item Пересекающиеся окна удаляются.
        \item Точки относятся к тому кластеру, центр которого находится ближе к ним.
    \end{enumerate}
\end{definition}

Преимущества Mean-Shift:
\begin{itemize}
    \item Число кластеров определяется автоматически
    \item Размеры кластеров могут быть сильно разными
    \item Центр кластера всегда в точке с локальным максимумом плотность, что обеспечивает устойчивость к выбросам.
\end{itemize}

Минусы:
\begin{itemize}
    \item Кластеры по-прежнему шарообразные (в контексте метрических пространств)
    \item Выбор размера окна может быть нетривиальной задачей и фактически этот параметр неявно задает число кластеров.
\end{itemize}

\subsection{DBSCAN}

\begin{definition}
    DBSCAN (Density-Based Spatial Clustering of Applications with Noise) --- наиболее яркий представитель алгоритмов плотностной кластеризации.
    \begin{enumerate}
        \item Алгоритм начинает работу с произвольной еще не посещенной точки.
        \item Если в $\varepsilon$-окрестности находится хотя бы minPoints других точек, то отмечаем данную точку как часть кластера и всю её $\varepsilon$-окрестность считаем этим кластером.
        \item Отмечаем точку как посещенную.
        \item Повторяем шаги 1 - 3 для всех точек.
        \item Точки, которые не попали ни в какой кластер, считаются шумом.
    \end{enumerate}
\end{definition}

Плюсы DBSCAN:
\begin{itemize}
    \item Число кластеров определяется автоматически
    \item Формы кластеров могут быть любыми
\end{itemize}

Минусы:
\begin{itemize}
    \item Требует настройку двух гиперпараметров, а не одного
    \item Для данных с разной плотностью в кластерах подбор гиперпараметров может быть сложным
\end{itemize}

\subsection{EM GMM}

\begin{definition}
    \textbf{EM GMM} (\textit{Expectation-Maximization Clustering using Gaussian Mixture Models} --- алгоритм кластеризации, который пытается гауссовскими распределениями покрыть точки плоскости, в остальном работает как k-Means.
\end{definition}

По сравнению с k-Means EM GMM имеет следующие плюсы:
\begin{itemize}
    \item Большая гибкость модели, так как кластеры могут быть эллиптической формы,
    \item Используются вероятности, то есть точка может принадлежать нескольким кластерам, а значит, можно стоить мягкую кластеризацию.
\end{itemize}

\subsection{Кластеризация на основе графов}

Довольно старая и тривиальная идея заключается в том, что бы представить объекты как вершины полносвязного графа $G=(v,e)$, в котором длины ребер равны расстояниям между объектами $\rho(v,u)$. Алгоритм кластеризации в данном случае заключается в удалении ребер, длина которых больше некоторого фиксированного радиуса $R$. Компоненты связности после удаления ребер и являются кластерами.

\subsection{Иерархическая кластеризация}

При иерархической кластеризации строятся дендрограммы --- деревья иерархии кластеров. Число кластеров определяется высотой дерева. Существуют два подхода:
\begin{enumerate}
    \item Агломеративный, когда маленькие кластеры постепенно объединяются в большие,
    \item Разделяющий, когда изначально один большой кластер постепенно дробится.
\end{enumerate}

\begin{remark}
    Агломеративный подход является значительно более эффективным, так как при разделении классов у нас есть слишком много вариантов это делать (порядка чисел Стирлинга и нет хорошего критерия выбора оптимального способа). 
\end{remark}

\begin{definition}
    \textbf{HAC} (\textit{Hierarchical Agglomerative Clustering} --- алгоритм кластеризации, который изначально считает каждую точку отдельным кластером и на каждом шаге сливает два ближайших кластера в один и повторяет этот процесс, пока не останется всего один кластер. Для подсчета расстояния между кластерами можно использовать довольно много различных подходов, вот некоторые из них:
    \begin{itemize}
        \item Average linkage --- среднее расстояние между точками обоих классов
        \item Расстояние между двумя ближайшими точками
        \item Расстояния между центроидами кластеров
    \end{itemize}
\end{definition}

Основной минус данного алгоритма заключается в том, что время его работы $O(n^3)$, зато он не требует задавать число кластеров и не чувствителен к выбору метрики.

\begin{remark}
    Данный алгоритм позволяет сначала выполнить построение дендрограммы единожды, а затем выбирать число кластеров без повторного обучения.
\end{remark}

\subsection{Спектральная кластеризация}

\begin{definition}
    \textbf{Спектральной кластеризация} --- это набор техник, которые используют спектр (собственные значения) матрицы сходства данных для осуществления снижения размерности перед кластеризацией в пространствах меньших размерностей. Матрица сходства подаётся в качестве входа и состоит из количественных оценок относительной схожести каждой пары точек в данных.
\end{definition}

\section{Нейросетевые методы кластеризации}

\subsection{Сквозная и двухэтапная кластеризация}

\begin{definition}
    \textbf{Двухэтапная кластеризация} (\textit{two-stage}), как следует из названия, производится в два этапа. Сначала необходимо получить эмбеддинги из замороженной глубокой нейросети или Word2Vec, а затем обработать их классическим (эвристическим) методом кластеризации.
\end{definition}

Преимущества такого подхода состоят в том, что все необходимые компоненты уже реализованы во многих библиотеках, а кластеризация интерпретируема. При этом двухэтапная кластеризация страдает от "проклятия размерности" и не учитывает априорную информацию.

\begin{definition}
    \textbf{Сквозные методы} (\textit{end-to-end}), в частности \textbf{глубокая кластеризация} (\textit{deep clustering}) --- это подход к решению задач кластеризации с помощью двухголовых нейронных сетей. В них есть кодировщик, первая голова -- декодировщик и вторая голова --- модуль, отвечающий за кластеризацию.
\end{definition}

Такие модели хорошо работают с высокой размерностью, учатся представлять данные в пространстве малой размерности, хорошо масштабируются, имеют довольно высокую скорость и точность. Однако гиперпараметры в них настраиваются нетривиально, к каждой задаче требуется строить сложную в реализации модель, а результаты тяжело интерпретировать.

\begin{remark}
    Функция ошибки в сквозных методах также усложняется, так как является линейной комбинацией ошибки сети и ошибки кластеризации.
\end{remark}

\begin{remark}
    Строить архитектуру глубокой нейронной сети для глубокой кластеризации можно на основе автокодировщика, на основе генеративной модели и с прямой оптимизацией кластеров.
\end{remark}
