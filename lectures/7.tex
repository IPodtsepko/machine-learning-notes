\chapter{Анализ текста}

Лектор: Илья Шамов

\section{Определение текста и построение эмбеддингов}

\begin{definition}
    \textbf{Текст} --- это набор слов. Если можем представить слово, то сумма представлений слов будет представлением текста.
\end{definition}

Основные тезисы об эмбеддингах текста:
\begin{itemize}
    \item One-hot encoding слов слишком ресурсоёмкий
    \item Ручная разметка слов невозможна
    \item Слово определяется контекстом
    \item Для получения эмбеддингов можно использовать следующие модели: skip-gram, CBOW
\end{itemize}

Задачи, связанные с анализом текстов:
\begin{itemize}
    \item Классификация
    \item Моделирование языка --- построение математической модели языка, эмулирующей естественный язык
    \item Генерация
    \item Перевод
    \item Выделение эмоций
    \item Ответы на вопросы, ведение диалога
    \item Суммаризация
    \item Поиск
    \item Анализ
\end{itemize}

\section{Предобработка текста средствами Python}

На этапе предварительной обработки текста можно выполнить следующие шаги (не все они являются оптимальными):
\begin{itemize}
    \item Удалить лишние символы, такие как знаки пунктуации.
    \item Посмотреть на статистику наиболее популярных слов. Скорее всего, в топе будут предлоги, артикли и т.д. Такие слова называются \textbf{стоп-словами}. Часть из них могут специфичными для набора данных.
    \item Далее можно очистить текст от общих для языка стоп-слов, выполнить это можно с помощью проверки всех слов на наличие в \texttt{nltk.corpus.stopwords} для необходимого языка (например, в \texttt{stopwords.words('english')}).
    \item После предыдущего шага в топе останутся специфичные для набора данных стоп-слова, может быть полезно избавиться и от них.
    \item Очень часто текст содержит разные формы одного и того же слова, полезно применять один из следующих подходов.
    \begin{definition}
        \textbf{Стемминг} (\texttt{nltk.stem.PorterStemmer}) --- замена слов на его основу.
    \end{definition}
    \begin{definition}
        \textbf{Лемматизация} (\texttt{nltk.stem.WordNetLemmatizer}) --- процесс приведения словоформы к лемме, то есть её нормальной (словарной) форме.
    \end{definition}
    \begin{remark}
        На практике для глубокого обучения чаще используют стемминг, так как он сильнее уменьшает размер словаря.
    \end{remark}
\end{itemize}

\section{Алгоритмы классического машинного обучения}

\begin{definition}
    \textbf{TF-IDF} (TF — term frequency, IDF — inverse document frequency) --- статистическая мера, используемая для оценки важности слова в контексте документа, являющегося частью коллекции документов или корпуса. Вес некоторого слова пропорционален частоте употребления этого слова в документе и обратно пропорционален частоте употребления слова во всех документах коллекции.
\end{definition}

TF-IDF строит эмбеддинги для текстов. Их размер равен размеру словаря и может быть порядка десятков тысяч элементов. По факту каждый тексты редко содержит все слова из словаря, поэтому если хранить такие эмбеддинги в разреженной матрице, то проблем не будет.

\begin{definition}
     \textbf{Latent Dirichlet allocation} (\textit{LDA}, \textit{Латентное размещение Дирихле}) --- применяемая в машинном обучении и порождающая unsupervised модель, позволяющая объяснять результаты наблюдений с помощью неявных групп, благодаря чему возможно выявление причин сходства некоторых частей данных. Например, если наблюдениями являются слова, собранные в документы, утверждается, что каждый документ представляет собой смесь небольшого количества тем и что появление каждого слова связано с одной из тем документа.
\end{definition}

Верхнеуровнево LDA описывается с помощью 2-х наборов параметров:
\begin{itemize}
    \item $\alpha$ --- вероятность каждого документа принадлежать каждому из топиков
    \item $\beta$ --- вероятность каждого слова принадлежать каждому из топиков
\end{itemize}

Модель случайно делит тексты на топики и далее с помощью градиентного спуска настраивает $\alpha$ и $\beta$. Таким образом она увеличивает правдоподобие своей оценки. Данная модель уже реализована: \texttt{gensim.models.LdaMulticore}.

\section{Обучение эмбеддингов слов}

Самый простой способ построения эмбеддингов для текстов --- это one-hot encoding, однако он несет слишком мало информации и на практике мало где применим. Рассмотрим более умные способы построения векторного представления текстов.

\begin{definition}
    \textbf{CBOW} (\textit{Continuous Bag of Words}) --- архитектура сети, решающей задачу определения наиболее вероятного слова по контексту.
\end{definition}

\begin{definition}
    \textbf{Word2Vec} — способ построения сжатого пространства векторов слов, использующий нейронные сети. Принимает на вход большой текстовый корпус и сопоставляет каждому слову вектор. Сначала он создает словарь, а затем вычисляет векторное представление слов. Векторное представление основывается на контекстной близости: слова, встречающиеся в тексте рядом с одинаковыми словами (а следовательно, имеющие схожий смысл).
\end{definition}

Еще один способ построения эмбеддингов --- использование глубоких сетей с архитектурой, разделенной на кодировщик и декодировшик. Если оставить только encoder --- то с помощью него можно строить эмбеддинги текстов. Примером такой архитектуры являются трансформеры, рассмотренные на предыдущей лекции.

\begin{definition}
    \textbf{BERT} — это многослойный двунаправленный кодировщик трансформер. В данной архитектуре используется двунаправленный механизм self-attention. Модель используется в совокупности с некоторым классификатором, на вход которого подается результат работы BERT — векторное представление входных данных. В основе обучения модели лежат две идеи:
    \begin{enumerate}
        \item 15\% слов заменяются масками и BERT учится их предсказывать.
        \item BERT учится определять, может ли одно предложение идти после другого.
    \end{enumerate}
\end{definition}

\begin{remark}
    В библиотеке \texttt{transformers} есть такие классы, как \texttt{BertTokenizer} и \texttt{BertModel}.
\end{remark}
