\chapter{Выбор признаков}

Лектор: Алексей Сергеевич Забашта

\section{Уменьшение размерности}

\begin{definition}
    Задача уменьшения размерности формулируется следующим образом: $\mathcal{F}=(f_1,...,f_n)$. Необходимо построить множество признаков $\mathcal{G}=(g_1,...,g_k):k<n$ (часто $k<<n$), переход к которым сопровождается наименьшей потерей информации.
\end{definition}

Цели уменьшения размерности:
\begin{enumerate}
    \item Ускорение обучения и обработки
    \item Борьба с шумом и мультиколлинеарностью
    \item Интерпретация и визуализация данных
\end{enumerate}

\begin{definition}
    \textbf{Проклятие размерности} (\textit{curse of dimensionality}) --- это набор проблем, возникающих с ростом размерности:
    \begin{enumerate}
        \item Увеличиваются требования к памяти и вычислительной мощности
        \item Данные становятся более разреженными
        \item Проще найти гипотезы, не имеющие отношения к реальности
    \end{enumerate}
\end{definition}

\begin{example}
    Когда-то ВВС некоторой страны решили, что удобнее шить форму не под каждого пилота, а под усредненного и брать только тех, кто подходит под эти параметры. Однако на практике оказалось, что поскольку параметров у такой формы много -- вероятность того, что хоть кто-то подойдет под все из них крайне мала.
\end{example}

Уменьшение размерности --- это один из шагов в предобработке данных:
\begin{itemize}
    \item Меньше памяти для хранения
    \item Уменьшение времени обработки
    \item Увеличение качества обработки
    \item Понимание природы признаков
\end{itemize}

\begin{figure}[h]
    \centering
    \includesvg[width=\textwidth]{\detokenize{./images/dimentionality-reduction-types.svg}}
    \caption{Методы уменьшения размерности}
\end{figure}

\begin{definition}
    \textbf{Выбор признаков} (\textit{feature selection}) --- методы, для которых $\mathcal{G}\subset\mathcal{F}$. Они:
    \begin{itemize}
        \item быстро работают;
        \item не могут "выдумывать" сложных признаков.
    \end{itemize}
    Их цель:
    \begin{enumerate}
        \item Уменьшение числа ресурсов, требуемых для обработки больших наборов данных;
        \item Поиск новых признаков.
    \end{enumerate}
\end{definition}

\begin{definition}
    \textbf{Извлечение признаков} (\textit{feature extraction}) --- все остальные методы, в том числе даже те, у которых $k>n$:
    \begin{itemize}
        \item в целом, дольше работают;
        \item могут извлекать сложные признаки.
    \end{itemize}
    Их цель:
    \begin{enumerate}
        \item Уменьшение переобучения и улучшение качества предсказания;
        \item Улучшение понимания моделей.
    \end{enumerate}
\end{definition}

Зачем вообще избавляться от признаков? Потому что они бывают ненужными:

\begin{definition}
    \textbf{Избыточные (redudant) признаки} не привносят дополнительной информации относительно остальных признаков.
\end{definition}

\begin{definition}
    \textbf{Нерелевантные (irrelevant) признаки} --- это неинформативные признаки.
\end{definition}

Методы выбора признаков подразделяются на:
\begin{enumerate}
    \item Встроенные методы (embedded)
    \item Фильтрующие методы (filter)
    \begin{itemize}
        \item Одномерные (univariate)
        \item Многомерные (multivatiate)
    \end{itemize}
    \item Методы-обертки (wrapper)
    \begin{itemize}
        \item Детерминированные (deterministic)
        \item Стохастические (stochastic)
    \end{itemize}
    \item Гибридные и ансамблирующие методы
\end{enumerate}

\section{Встроенные методы}

\begin{figure}[h]
    \centering
    \includesvg[width=\textwidth]{\detokenize{./images/embeeded-fs.svg}}
    \caption{Схема встроенного метода}
\end{figure}

\begin{definition}
    \textbf{Встроенные методы} (\textit{embeeded methods}) --- это методы выбора признаков, при которых этот выбор осуществляется в процессе работы других алгоритмов (классификаторов и регрессоров):
    \begin{itemize}
        \item Опираются на конкретный алгоритм и его реализацию
        \item Специфичны для каждого алгоритма
    \end{itemize}
\end{definition}

\begin{example}
    \textbf{SVM-RFE} (\textit{Support Vector Machine Recursive Feature Elimination}) --- типичный встроенный метод:
    \begin{itemize}
        \item Обучается SVM на тренировочном подмножестве набора данных.
        \item Веса признаков устанавливаются равными модулям соответствующих коэффициентов.
        \item Признаки ранжируются согласно их весам.
        \item Некоторое число признаков с наименьшими весами выбрасываются.
        \item Шаги 1 - 4 повторяют, пока не останется необходимое число признаков.
    \end{itemize}
    Данный метод считается встроенным именно потому что он завязан на внутреннем устройстве SVM. А именно --- на существовании некоторых коэффициентов по абсолютной величине которых можно оценивать важность признаков.
\end{example}

\begin{example}
    Выбор признаков на основе случайного леса --- еще один встроенный метод. Он учитывает число и глубину вхождений признака в деревья.
\end{example}

\section{Методы-обертки}

\begin{figure}[h]
    \centering
    \includesvg[width=\textwidth]{\detokenize{./images/wrapper-fs.svg}}
    \caption{Схема метода-обертки}
\end{figure}

\begin{definition}
    \textbf{Методы-обертки} ( \textit{wrapper methods}) --- методы выбора признаков, которые используют алгоритм (классификатор или регрессор) для оценки качества получаемого подмножества признаков и алгоритмы дискретной оптимизации для поиска оптимального подмножества признаков.
\end{definition}

\textbf{Классификация методов оберток}
\begin{enumerate}
    \item Детерминированные:
    \begin{itemize}
        \item SFS (sequential forward selection) -- итеративный алгоритм, который постепенно выбирает наиболее полезные признаки. Сначала рассматриваются все признаки по отдельности: $f_1$, ..., $f_n$. Среди них выбирается тот, на котором классификатор или регрессор показывает наилучший результат: $f_{i_1}$. Затем рассматриваются пары признаков $f_{i_1}, f_{i_2\ne i_1}$. Среди всех таких пар выбирается лучшая и так далее.
        \item SBE (sequential backward elimination) --- алгоритм аналогичный SFS, но начинающий с полного набора признаков и выкидывающий на каждой итерации наименее полезный признак -- то есть тот, без которого классификатор или регрессор показывает наилучший результат.
        \item Перестановочная полезность (permutation importance) --- метод, который обучает какой-то алгоритм (классификатор или регрессор) на некоторой части $D$ набора данных c использованием всех признаков, далее для каждого признака рассчитывает меру его полезности $\mu(f_i)$ согласно следующим правилам: 
        \begin{enumerate}
            \item Строится новый набор данных $D_i$, который получается из $D$ перестановкой значений $f_i$ между объектами.
            \item $\mu(f_i)=\mathcal{L}(D)-\mathcal{L}(D_i)$, где $\mathcal{L}$ -- функция, которая принимает набор данных и возвращает величину ошибки предсказания обученной ранее модели на этом наборе данных. Таким образом, если ошибка увеличивается, то значение $\mu(f_i)$ будет падать.
        \end{enumerate}
        Признаки, мера $\mu$ перестановочной полезности которых оказалась наименьшей, выкидываются.
        \begin{remark}
            Признаки нужно "портить" именно перестановкой, чтобы сохранить статистические показатели, такие как $\E$ и $\D$.
        \end{remark}
    \end{itemize}
    \item Стохастические --- сводят задачу выбора признаков к задаче оптимизации в пространстве бинарных векторов:\
    \begin{itemize}
        \item Поиск восхождением на холм (почти аналогичен SFS)
        \item Генетические алгоритмы
    \end{itemize}
\end{enumerate}

Преимуществами методов-оберток являются:
\begin{itemize}
    \item Более высокая точность, чем у фильтров;
    \item Использование отношений между признаками;
    \item Оптимизация качества предсказательной модели в явном виде.
\end{itemize}

Однако, у них есть ряд недостатков:
\begin{itemize}
    \item Очень большое время работы;
    \item Склонность к переобучению при неправильной работе с разбиением набора данных.
\end{itemize}

\section{Фильтры}

\begin{figure}[h]
    \centering
    \includesvg[width=\textwidth]{\detokenize{./images/filter-fs.svg}}
    \caption{Схема фильтрующих методов}
\end{figure}

\begin{definition}
    \textbf{Фильтры} (\textit{filter methods}) --- методы, которые оценивают качество отдельных признаков или подмножеств признаков и удаляют худшие. Состоят из двух компонент:
    \begin{enumerate}
        \item Меры значимости признаков $\mu$;
        \item Правила обрезки $\kappa$, которое определяет, какие признаки необходимо удалить на основе $\mu$.
    \end{enumerate}
\end{definition}

\textbf{Классификация фильтрующих методов}
\begin{enumerate}
    \item Одномерные
    \begin{itemize}
        \item Между двумя вещественными числами:
        \begin{itemize}
            \item Евклидово расстояние
            \item Коэффициент корреляции (Пирсона или Спирмена)
        \end{itemize}
        \item Между вещественным числом и категорией:
        \begin{itemize}
            \item Попарные расстояния (внутренние или внешние)
            \item Условная дисперсия
        \end{itemize}
        \item Между двумя категориями:
        \begin{itemize}
            \item Прирост информации (IG)
            \item Индекс Джини
            \item $\chi^2$
        \end{itemize}
    \end{itemize}
    \item Многомерные
    \begin{itemize}
        \item Выбор признаков на основе корреляций (CFS)
        \item Фильтр марковского одеяла (MBF)
    \end{itemize}
\end{enumerate}

\begin{definition}
    \textbf{Коэффициент корреляции Пирсона}
    \[
        r=\dfrac{\sum_{i,j}(x_{ij}-\overline{x_j})(y_i-\overline{y})}{\sqrt{\sum_{i,j}(x_{ij}-\overline{x_j})^2\sum_i(y_i-\overline{y})^2}}\in[-1;1]
    \]
\end{definition}

\begin{definition}
    \textbf{Коэффициент корреляции Спирмена} рассчитывается следующим алгоритмом:
    \begin{enumerate}
        \item Отсортировать объекты двумя способами (по каждому из признаков);
        \item Найти ранги объектов для каждой сортировки;
        \item Вычислить \textit{корреляцию Пирсона} между векторами рангов.
    \end{enumerate}
\end{definition}

В качестве правила обрезки $\kappa$ можно брать:
\begin{enumerate}
    \item Число признаков;
    \item Порог значимости признаков;
    \item Интегральный порог значимости признаков;
    \item Метод сломанной трости;
    \item Метод локтя.
\end{enumerate}

Одномерные фильтры очень быстро работают и позволяют оценивать значимость каждого признака, однако они игнорируют отношения между признаками и то, что реально использует предсказательная модель.\newline
Многомерные фильтры работают существенно медленнее одномерных, но все еще гораздо быстрее, чем методы-обертки. Их преимущество по сравнению с одномерными фильтрами состоит в том, что они учитывают отношения между признаками.

\section{Гибриды и ансамбли}

Можно попробовать использовать сильные стороны разных подходов, комбинируя их. Наиболее частый вариант:
\begin{enumerate}
    \item Применение фильтра или набора фильтров к исходному набору признаков с целью отсеивания лишних;
    \item Применение метода-обертки или встроенных методов к оставшимся признакам.
\end{enumerate}

Выделяются следующие способы комбинирования методов выбора признаков (см. рис. 10.5, 10.6):
\begin{enumerate}
    \item Гибридный подход
    \item Ансамблирование при выборе признаков
    \item Ансамблирование на уровне моделей
    \item Ансамблирование на уровне ранжирования
    \item Ансамблирование на уровне мер значимости
\end{enumerate}

\begin{figure}[h]
    \centering
    \includesvg[width=\textwidth]{\detokenize{images/ensembling-fs-1.svg}}
    \caption{а) Схема гибридного подхода, б) схема ансамблирования в выборе признаков, в) схема ансамблирования на уровне моделей}
\end{figure}

\begin{figure}[h]
    \centering
    \includesvg[width=\textwidth]{\detokenize{images/ensembling-fs-2.svg}}
    \caption{а) Схема ансамблирования на уровне ранжирования, б) схема ансамблирования на уровне мер значимости}
\end{figure}

Применение гибридных и ансамблируюших методов чаще всего лучше по времени и качеству, однако интерпретируемость результата может снижаться. К тому же повышается риск переобучения.