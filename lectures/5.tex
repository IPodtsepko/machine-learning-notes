\chapter{Фреймворки для глубокого обучения}

Лекторы: Алексей Сергеевич Забашта и Илья Шамов

\section{Краткий обзор популярных фреймворков}

В Python-библиотеке \texttt{sklearn} есть класс \texttt{sklearn.neural\_network.MLPClassifier}, который предоставляет интерфейс для работы с полносвязными нейронными сетями, однако он достаточно примитивен, так как не позволяет достаточно гибко настраивать сеть и её обучение:
\begin{enumerate}
    \item Методы оптимизации выбираются из небольшого списка
    \item Сходу не понятно, как настроить функцию ошибки
\end{enumerate}

Если требуется что-то более серьезное, то можно использовать библиотеку \texttt{PyTorch}. Данный фреймворк поддерживает возможность более гибкой настройки и вычисления на видеокарте. Основная "магия" в \texttt{PyTorch} происходит благодаря поддержке автоматического дифференцирования и возможности гибко настраивать, для каких нейронов необходимо рассчитывать градиент (это позволяет замораживать слои).
\begin{remark}
    \texttt{PyTorch} предоставляет интерфейс для создания собственных блоков. "Что-то обучаемое" необходимо наследовать от \texttt{torch.nn.Module} и переопределять в нем функцию \texttt{forward}. Функция \texttt{backward} и список параметров модели определяются автоматически при проходе графа вычислений.
\end{remark}

Альтернативой библиотеке \texttt{PyTorch} является \texttt{TensorFlow}. Он поддерживает работу со статическим графом вычислений и с динамическим (при использовании Eager API). Во второй версии библиотеки также есть возможность наследовать обучаемую модель от \texttt{tensorflow.keras.Model} и удобно работать с динамическим графом (интерфейс в TensorFlow аналогичный, вместо метода \texttt{forward} есть метод \texttt{call}).

 \section{Подробнее о PyTorch}

 \begin{remark}
     Между \texttt{PyTorch} от Facebook и \texttt{TensorFlow} от Google шло медиасражение и PyTorch выиграл, так как в TensorFlow изначально продвигали статический граф, а затем сделали две версии библиотеки и с этим еще надо разобраться. 
 \end{remark}

Алгоритм создания и обучение модели на \texttt{PyTorch}:
\begin{enumerate}
    \item Модель необходимо наследовать от \texttt{torch.nn.Module}
    \item В методе \texttt{\_\_init\_\_} необходимо перечислить и сохранить слои модели. Для создания слоев предоставляется широкий набор инструментов. Некоторые из них:
    \begin{itemize}
        \item \texttt{torch.nn.Sequential}, позволяющий перечислить слои, которые должны применяться последовательно и инкапсулирующий в себе их применение
        \item \texttt{torch.nn.Conv2d} --- свёрточный слой
        \item \texttt{torch.nn.Linear} --- полносвязный слой
        \item \texttt{torch.nn.ReLU} --- функция активации ReLU
        \item \texttt{torch.nn.MaxPool2d} --- пулинг с операцией $\max$
    \end{itemize}
    \item Далее определяется метод \texttt{forward}, в котором вызываются слои модели, сохраненные в конструкторе (благодаря вызову слоев, будет строиться динамический граф вычислений).
    \item Для загрузки данных можно использовать интерфейс \texttt{torch.utils.data.DataLoader}, который может инкапсулировать постепенную загрузку данных небольшими частями, которые помещаются в оперативную память компьютера (можно определять свои \texttt{DataLoader}'ы).
    \item Для проверки структуры и оценки кол-ва параметров модели используется специальная библиотека \texttt{torchsummaryX}, предоставляющая фукнцию \texttt{summary}.
    \begin{remark}
        Поскольку в \texttt{PyTorch} используется динамический граф, в функцию \texttt{summary} также требуется передавать пример входных данных. Для трехканальных изображений 32 на 32 пискеля его можно создать с помощью следующего метода: \texttt{torch.ones(1, 3, 32, 32)}.
    \end{remark}
    \item Для обучения модели используются оптимизаторы, например, \texttt{torch.optim.Adam}. Обычно они принимают параметры модели (\texttt{model.parameters()}) и гиперпараметры для алгоритма оптимизации.
    \item Обучение модели частично не инкапсулировано, так что необходимо \textit{самостоятельно} запускать внешний цикл по эпохам и внутренний по \texttt{DataLoader}'у, обнулять градиент у оптимизатора методом \texttt{zero\_grad}, запускать вычисление оптимизируемой функции и подсчет градиента, а также оценивать результат модели для текущего шага и переходить к следующему шагу оптимизации (метод \texttt{step} оптимизатора). Такой подход обеспечивает очень большую гибкость процесса обучения, без большой нагрузки на разработчика, так как минимальный вариант кода, реализующего обучение записывается в 5 -- 10 строчек. 
\end{enumerate}

\begin{remark}
    Для логирования данных об обучении модели рекомендуется использовать библиотеку \texttt{tensorboardX}, который предоставляет интерфейс \texttt{SummaryWriter}. В результате создается специальный файл в отдельной директории, который можно открыть утилитой \texttt{tensorboard} (\texttt{\%tensorboard -{}-logdir runs}). Она удобно отображает графики и позволяет настраивать сглаживание, цвета и так далее.
\end{remark}
