\chapter{Настройка гиперпараметров}

Лектор: Алексей Сергеевич Забашта

\section{Выбор алгоритма и настройка гиперпараметров}

\begin{figure}[h]
    \centering
    \includesvg[scale=0.9]{\detokenize{./images/algorithm-training-scheme.svg}}
    \caption{Схема обучения алгоритма}
\end{figure}

\begin{definition}
Различные определения \textbf{гиперпараметров}:
\begin{itemize}
    \item Параметры алгоритма обучения.
    \item Параметры, которые не меняются во время обучения.
    \item Параметры которые можно установить до наблюдения набора данных.
    \item Структурные параметры.
\end{itemize}
\end{definition}

\begin{definition}
    \textbf{Задача настройки гиперпараметров} строится следующим образом. Пусть:
    \begin{itemize}
        \item $\mathcal{D}$ --- набор данных
        \item $A$ --- алгоритм обучения
        \item $p$ --- гиперпараметры алгоритма обучения
        \item $\mathcal{L}$ --- функция ошибки (валидация)
    \end{itemize}
    Требуется найти $p_{\mathrm{best}} = \mathrm{HyperParameterTuning}(A, \mathcal{D}, \mathcal{L}) = \underset{p}{\arg\min}\mathcal{L}(A_p, \mathcal{D})$
\end{definition}

Типичный вид алгоритма обучения гиперпараметров выглядит как перебор гиперпараметров по некоторому алгоритму, проверка результата на данных и тестирование модели на валидационной части данных. Фактически это тоже алгоритм обучения, просто более сложный.

\begin{definition}
    \textbf{Задача выбора алгоритма} --- для каждого алгоритма требуется выбрать наилучший для него алгоритм обучения.
Данную проблему формализовал Rice J. R. в своей статье "The algorithm selection problem":
\[
A_{\mathrm{best}} = \mathrm{AlgorithmSelection}(\mathcal{D}, \mathcal{L}) = \underset{A}{\arg\min}\mathcal{L}(A, \mathcal{D})
\]
\end{definition}

\begin{remark}
    Задачи настройки гиперпараметров и выбора алгоритма хорошо сводятся друг к другу, так как алгоритм можно считать гиперпараметром, а алгоритмом можно считать алгоритм с фиксированными параметрами. Однако второй подход, например, может приводить к очень сильному разрастанию числа алгоритмов.
\end{remark}

На практике требуется выполнять разделение задач поиска алгоритма и классификации, несмотря на то, что они сводятся друг к другу:
\begin{itemize}
    \item Гиперпараметры как и признаки можно типизировать числом и категорией.
    \item Можно отнести категориальные гиперпараметры к "алгоритмам", а числовые - к "гиперпараметрам".
\end{itemize}

\begin{remark}
    Может показаться логичной идея разделять "как есть", однако некоторые алгоритмы очень сильно изменяются при различных гиперпараметрах, например
    \begin{itemize}
        \item Функция предсказания kNN с априорными весами $w$:
        \[
            \mathrm{class}(x') = \mathrm{sign}\left(\sum_{(x,y)\in\mathcal{D}_\mathrm{train}} y\cdot w(x)\cdot\kappa\left(\dfrac{\rho\left(x', x\right)}{h}\right)\right)
        \]
        \item Функция предсказания SVM в общем виде:
        \[
            \mathrm{class}(x') = \mathrm{sign}\left(\sum_{(x,y)\in\mathcal{D}_\mathrm{train}}y\cdot\lambda(x)\cdot\kappa(x', x)\right)
        \]
        \item Функция предсказания SVM с линейным ядром:
        \[
            \mathrm{class}(x') = \mathrm{sign}\left(\sum_j \alpha_j\cdot x'_j\right)
        \]
    \end{itemize}
    Таким образом, алгоритм с одними гиперпараметрами может быть больше похож на другой алгоритм, чем на такой же алгоритм, но с другими гиперпараметрами.
\end{remark}

\section{Конфигурация гиперпараметров}

Мы будем рассматривать два типа конфигурации гиперпараметров:
\begin{itemize}
    \item линейную
    \item древовидную
\end{itemize}

На рис. 1.2 представлен пример конвейера, который подбирает гиперпараметры для моделей всех этапов обработки данных:

\begin{figure}[h]
    \centering
    \includesvg[scale=0.9]{\detokenize{./images/linear-configuration-of-algorithms-combination.svg}}
    \caption{Pipeline преобразований}
\end{figure}

Древовидная конфигурация (рис. 1.3) более естественная, так как перебираются параметры, необходимые для конкретного алгоритма.

\begin{figure}[h]
    \centering
    \includesvg[scale=0.8]{\detokenize{./images/tree-like-configuration-of-hyperparameters.svg}}
    \caption{Пример древовидной конфигурации}
\end{figure}

На рис. 1.4 в общем виде представлена схема древовидной конфигурации.

\begin{figure}[h]
    \centering
    \includesvg[scale=0.8]{\detokenize{./images/tree-like-scheme.svg}}
    \caption{Общий вид древовидной конфигурации}
\end{figure}


\section{Поиск по сетке}

\begin{definition}
    \textbf{Grid search} (\textit{поиск по сетке}) --- алгоритм при котором перебираются все возможные комбинации значений (числовые гиперпараметры дискретизируются), которые образуют декартово произведение списков значений гиперпараметров (сетку).
\end{definition}

\begin{remark}
    Комбинации значений можно получать на лету, не тратя много памяти, однако поиск по сетке работает очень долго. Причем главная проблема здесь еще и в том, что время его работы трудно рассчитать, если оно зависит от гиперпараметров. Если после поиска время осталось, значит была взята слишком "крупная" сетка. Если времени не хватило, значит были рассмотрены не все комбинации, и такой промежуточный результат будет предвзят к порядку гиперпараметров.
\end{remark}

\begin{remark}
    Использование древовидной конфигурации может значительно уменьшить кол-во комбинаций, требующих перебора.
\end{remark}

В \texttt{sklearn} есть реализация \texttt{GridSearchCV}. 

\section{Случайный поиск}

\begin{definition}
    \textbf{Random search} (\textit{случайный поиск}) ---  алгоритм, при котором значения гиперпараметров задаются случайными распределениями. На каждой итерации комбинация значений гиперпараметров выбирается случайно. Число итераций ограничивается лимитом вычислений.
\end{definition}

В том же \texttt{sklearn} есть \texttt{RandomizedSearchCV}, реализующий алгоритм случайного поиска гиперпараметров.\\

\textbf{Анализ случайного поиска}:
\begin{itemize}
    \item Может работать с гиперпараметрами у которых бесконечное число значений.
    \item Не требуется использовать древовидную конфигурацию.
    \item Проще рассчитать время работы, так как процесс оптимизации можно остановить в любой момент.
    \item Плохо работает с категориальными гиперпараметрами, так как некоторые комбинации могут повторяться, а некоторые могут быть вообще не рассмотрены. Например, для получения всех $n$ значений требуется в среднем $O(n\log n)$ повторов.
    \item Получается, что случайный поиск работает плохо там, где поиск по сетке работает хорошо.
\end{itemize}

\section{Эволюционные алгоритмы}

\begin{definition}
    \textbf{Black-Box Optimization} (\textit{оптимизация "черного ящика"}) --- задача, при которой алгоритм оптимизации может вычислять функцию ошибки $\mathcal{L}$ на произвольном входе из её области определения, но не может получить никакой дополнительной информации о данной функции. Как правило, применяется для не дифференцируемых, не выпуклых или медленно вычислимых функций. 
\end{definition}

\begin{definition}
    \textbf{Критерии остановки} и сравнения алгоритмов оптимизации:
    \begin{itemize}
        \item Минимум $\mathcal{L}$ при фиксированном времени или числе вызовов $\mathcal{L}$.
        \item Количество времени или число вызовов $\mathcal{L}$ для достижения требуемого значения $\mathcal{L}$.
    \end{itemize}
\end{definition}

Для оптимизации абстрактных объектов помимо функции ошибки (\textit{приспособленности}) $\mathcal{L}(x)$ иногда нужны операторы:
\begin{enumerate}
    \item \textbf{Генерации} - создает новый объект: \[\mathrm{rand}():\quad X\]
    \item \textbf{Мутации} - делает из объекта $x$ новый объект $x'$, который похож на $x$: \[\mathrm{mutate}(x):\quad X\to X\]
    \item \textbf{Кроссовера} (кроссинговера) - делает из двух объектив третий, который одновременно похож на первый и второй: \[\mathrm{cross}(x_1, x_2):\quad X \times X \to X\]
\end{enumerate}

Примеры абстрактный алгоритмов:
\begin{enumerate}
    \item \textbf{Random search} (\textit{случайный поиск}) использует только оператор случайной генерации. Является базовым (наивным) решением задачи оптимизации.
    \item \textbf{Hill climbing} (\textit{поиск с восхождением к вершине}) делает случайные изменения (мутации) в надежде улучшить результат.
    \item \textbf{Simulated annealing} (\textit{алгоритм имитации отжига}) похож на предыдущий, но иногда может переходить в более плохое состояние.
    \item \textbf{Генетический алгоритм} использует сразу несколько объектов (популяцию) для оптимизации. Помимо мутации скрещивает объекты кроссовером.
\end{enumerate}

Вместо абстрактного пространства рассматривается $X = \mathbb{R}^m$, так как с числами можно производить больше операций.

\begin{remark}
    Численные методы являются частным случаем абстрактных.
\end{remark}

\begin{example}
    Переход к абстрактной оптимизации
    \[\mathrm{rand}() = (\mathcal{N}(0, 1), ..., \mathcal{N}(0, 1))\]
    \[\mathrm{mutate}(x) = x + (\mathcal{N}(0, \epsilon), ..., \mathcal{N}(0, \epsilon))\]
    \[\mathrm{cross}(x_1, x_2) = \dfrac{x_1 + x_2}{2}\]
    \begin{remark}
        Вместо небольшого изменения каждой координаты, можно менять случайное подмножество координат.
    \end{remark}
\end{example}

Функций ошибки может быть несколько, например, ошибка решения и время работы. Нельзя просто суммировать величины с разными размерностями, однако можно находить оптимум по какой-нибудь функции ошибки при фиксированных значениях остальных. Результатом многокритериальной оптимизации является \textbf{парето-фронт} решений.

\begin{figure}[h]
    \centering
    \includesvg[scale=1.1]{\detokenize{./images/pareto-front.svg}}
    \caption{Парето-фронт}
\end{figure}

\begin{remark}
    Для применения алгоритмов поиска в числовых пространствах требуется функция, которая будет отображать вектор чисел в комбинацию гиперпараметров. Например, если требуется сделать из чисел категорию, можно использовать стандартное преобразование: onehot-encoding делает из категории число, а argmax - из числа категорию. Однако при модификации вектора чисел, если положение максимума не изменится, то в результате получится то же значение категории и значение функции ошибки не изменится. Чтобы не допускать такие проблемы лучше использовать смешанное пространство поиска:
    \begin{itemize}
        \item При мутации категории её значение заменяется на случайное другое.
        \item При кроссовере двух категорий случайно выбирается одно из двух значений.
        \item При мутации целого числа к нему прибавляется целочисленный сдвиг.
        \item При кроссовере двух целых чисел можно брать среднее и округлять.
    \end{itemize}
\end{remark}

\section{Реализация методов по настройке гиперпараметров}

Популярной библиотекой на языке \texttt{Python} для настройки гиперпараметров является библиотека \texttt{optuna}. Её можно использовать не только для настройки гиперпараметров. Для её использования достаточно реализовать функцию ошибки (\texttt{objective}), которая будет зависеть от текущей конфигурации (\texttt{trial}).